### Normalization

The raw $\mathbf{G}_{f}$ matrix for all of the above GRMs undergoes a normalization step before being saved:
The matrix $\mathbf{G}_{f}$ is then divided by the mean of its diagonal elements:
    $$\mathbf{G}_{f,norm} = \frac{\mathbf{G}_{f}}{tr(\mathbf{G}_{f})/N}$$
This normalization ensures that the average of the diagonal elements of the final $\mathbf{G}_{f,norm}$ matrix is 1 and suitable for variance component estimation.

### Algorithmic and Implementation Choices

The computation of these GRMs involves handling potentially a large number of SNPs or unique haplotypes per block and many blocks across the genome. Several key strategies for efficiency and scalability with large genomic datasets:

- **Parallel Processing (MPI)**: The Message Passing Interface (MPI) is used to distribute the computational workload across multiple processors or compute nodes. This allows for parallel execution of data reading, matrix construction, and calculations.
- **ScaLAPACK Library**: For high-performance distributed linear algebra operations, the software utilizes ScaLAPACK (Scalable Linear Algebra PACKage) routines, often via the Intel MKL implementation.
- Each MPI rank builds its own chunk of $\mathbf{W}_i$ matrices (only reads and assembles the corresponding columns for the set of SNPs or haplotype blocks assigned to it). The local chunks are then aggregated into a global block 2d block‚Äêcyclic distributed matrix for subsequent calculations so that no single process ever holds all $n \times N$ or $n \times n$ resulting matrix. As a result, both memory and compute scale well with the number of mpi tasks (ranks). 
- **Iterative Accumulation over Genomic Segments**: GRMs are typically constructed by processing the genome in segments (e.g., per chromosome or large blocks of markers). Intermediate $\mathbf{W}_{segment}\mathbf{W}_{segment}^T$ matrices are computed for each segment and summed up: $\mathbf{G} = \sum_{segment} \mathbf{W}_{segment}\mathbf{W}_{segment}^T$. This approach manages memory effectively and allows for parallel processing.
- **Hadamard Products for Interaction GRMs**: Second-order interaction GRMs ($\mathbf{G}_{AA}, \mathbf{G}_{AD}, \mathbf{G}_{DD}$) are computed using element-wise (Hadamard) products of the first-order GRMs using local matrix chunks in parallel. This is computationally much more efficient than explicitly forming large interaction $\mathbf{W}$ matrices, takes very little memory and can be done on a regular computer using all of the available cores for parallelization.
- **Missing Data Handling**: Genotypes or haplotypes marked as missing (e.g., coded as 0 for haplotypes or a value other than 0, 1, 2 for SNPs in the raw input) are typically assigned a score of 0 in the intermediate $\mathbf{W}$ matrices. This means they do not contribute to the relationship scores for those specific alleles or SNPs, effectively being an imputation to the mean before centering.
